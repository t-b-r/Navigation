{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Navigation Project\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### 1. Import Necessary Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Load and explore the environment\n",
    "\n",
    "The simulation contains a single agent that navigates a large environment.  At each time step, it has four actions at its disposal:\n",
    "- `0` - walk forward \n",
    "- `1` - walk backward\n",
    "- `2` - turn left\n",
    "- `3` - turn right\n",
    "\n",
    "The state space has `37` dimensions and contains the agent's velocity, along with ray-based perception of objects around agent's forward direction.  A reward of `+1` is provided for collecting a yellow banana, and a reward of `-1` is provided for collecting a blue banana. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: BananaBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 37\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: discrete\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 1\n",
      "Number of actions: 4\n",
      "States look like: [1.         0.         0.         0.         0.84408134 0.\n",
      " 0.         1.         0.         0.0748472  0.         1.\n",
      " 0.         0.         0.25755    1.         0.         0.\n",
      " 0.         0.74177343 0.         1.         0.         0.\n",
      " 0.25854847 0.         0.         1.         0.         0.09355672\n",
      " 0.         1.         0.         0.         0.31969345 0.\n",
      " 0.        ]\n",
      "States have length: 37\n"
     ]
    }
   ],
   "source": [
    "# load the environment\n",
    "env = UnityEnvironment(file_name=\"/home/taylor/Classes/deep_rl/projects/deep-reinforcement-learning/p1_navigation/Banana_Linux/Banana.x86_64\")\n",
    "\n",
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "\n",
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents in the environment\n",
    "print('Number of agents:', len(env_info.agents))\n",
    "\n",
    "# number of actions\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Number of actions:', action_size)\n",
    "\n",
    "#get the current state\n",
    "state = env_info.vector_observations[0]\n",
    "print('States look like:', state)\n",
    "state_size = len(state)\n",
    "print('States have length:', state_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Load the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dqn_agent import Agent\n",
    "#initialize the agent - # of states, actions, and a seed of 0\n",
    "agent = Agent(state_size=37, action_size=4, seed=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Train the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode #: 1 t:  0 Score:  0.0 done:  False\n",
      "Episode #: 1 t:  1 Score:  0.0 done:  False\n",
      "Episode #: 1 t:  2 Score:  0.0 done:  False\n",
      "Episode #: 1 t:  3 Score:  0.0 done:  False\n",
      "Episode #: 1 t:  4 Score:  0.0 done:  False\n",
      "Episode #: 1 t:  5 Score:  0.0 done:  False\n",
      "Episode #: 1 t:  6 Score:  0.0 done:  False\n",
      "Episode #: 1 t:  7 Score:  0.0 done:  False\n",
      "Episode #: 1 t:  8 Score:  0.0 done:  False\n",
      "Episode #: 1 t:  9 Score:  0.0 done:  False\n",
      "Episode #: 1 t:  10 Score:  0.0 done:  False\n",
      "Episode #: 1 t:  11 Score:  0.0 done:  False\n",
      "Episode #: 1 t:  12 Score:  0.0 done:  False\n",
      "Episode #: 1 t:  13 Score:  0.0 done:  False\n",
      "Episode #: 1 t:  14 Score:  0.0 done:  False\n",
      "Episode #: 1 t:  15 Score:  0.0 done:  False\n",
      "Episode #: 1 t:  16 Score:  0.0 done:  False\n",
      "Episode #: 1 t:  17 Score:  0.0 done:  False\n",
      "Episode #: 1 t:  18 Score:  0.0 done:  False\n",
      "Episode #: 1 t:  19 Score:  0.0 done:  False\n",
      "Episode #: 1 Score:  0.0\n",
      "1\n",
      "0.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def dqn(n_episodes = 3, max_t = 20, eps_start = 1.0, eps_end = 0.01, eps_decay = 0.995):\n",
    "    \"\"\" Deep Q learning for Navigations Project\n",
    "    Params:\n",
    "        n_episodes (int): max n umber of training episodes\n",
    "        max_t (int): max n umber of time steps per episode\n",
    "        eps_start (oat): startign vbalue for epsilon, for the epsilon greedy action section\n",
    "        eps_end (float): min value of epsilon\n",
    "        eps_decay (float): multiplicative factor (per episode) for decreasing epsilon    \n",
    "    \"\"\"\n",
    "    scores = [] # list containing scores for each episode\n",
    "    scores_window = deque(maxlen=100) # last 100 scores\n",
    "    eps = eps_start # initialize epsilon\n",
    "    reward = 0\n",
    "    for i_episode in range(1, n_episodes +1): \n",
    "        env_info = env.reset(train_mode=True)[brain_name]\n",
    "        state = env_info.vector_observations[0]\n",
    "        score = 0\n",
    "        for t in range(max_t):\n",
    "            action = agent.act(state, eps)                 # choose an action                 \n",
    "            env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "            next_state = env_info.vector_observations[0]   # get the next state\n",
    "            reward = env_info.rewards[0]                   # get the reward\n",
    "            done = env_info.local_done[0]                  # see if episode has finished\n",
    "            agent.step(state, action, reward,  next_state, done) # step into next state and return the resulting state\n",
    "            score+= reward                                 # increment the score\n",
    "            print(\"Episode #:\", i_episode, \"t: \", t, \"Score: \", score, \"done: \", done)\n",
    "            if done:                                       # if you have reached the goal, then exit the loop\n",
    "                break\n",
    "\n",
    "        print(\"Episode #:\", i_episode, \"Score: \", score)\n",
    "        scores_window.append(score) #save most recent score\n",
    "        scores.append(score) # save most recent score\n",
    "        eps = max(eps_end, eps_decay*eps) #decrease epsilon\n",
    "        print(i_episode)\n",
    "        # print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)), end=\"\")\n",
    "\n",
    "        if i_episode % 100 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "        \n",
    "        print(np.mean(scores_window))\n",
    "        if np.mean(scores_window)>=1.0:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_window)))\n",
    "            torch.save(agent.qnetwork_local.state_dict(), 'checkpoint.pth')\n",
    "            break\n",
    "        return scores\n",
    "\n",
    "scores = dqn()\n",
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Close the environment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
